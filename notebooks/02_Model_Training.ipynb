{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f0c028",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "# --- PATH AYARLARI (Local Yapıya Uygun) ---\n",
    "# Eğer Colab'da çalışacaksan bu kısmı değiştirip dosyaları yanına yüklemen yeterli.\n",
    "# Localde: notebooks klasörünün bir üstüne çıkıp input'a gidiyoruz.\n",
    "current_dir = os.getcwd()\n",
    "# Eğer notebook'tan çalışıyorsan ../input, script ise os.path mantığı\n",
    "INPUT_DIR = os.path.join(os.path.dirname(current_dir), \"input\") \n",
    "OUTPUT_DIR = os.path.join(os.path.dirname(current_dir), \"output\", \"submissions\")\n",
    "\n",
    "# Klasör yoksa oluştur\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# --- DONANIM AYARLARI ---\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 15\n",
    "LR = 0.001\n",
    "NUM_CLASSES = 300\n",
    "\n",
    "print(f\"Cihaz: {DEVICE}\")\n",
    "print(f\"Veri Yolu: {INPUT_DIR}\")\n",
    "\n",
    "# 1. Dosyaları Yükle\n",
    "print(\"Veriler belleğe yükleniyor...\")\n",
    "try:\n",
    "    train_emb = np.load(os.path.join(INPUT_DIR, \"train_embeddings.npy\"), allow_pickle=True)\n",
    "    train_ids = np.load(os.path.join(INPUT_DIR, \"train_ids.npy\"), allow_pickle=True)\n",
    "    test_emb = np.load(os.path.join(INPUT_DIR, \"test_embeddings.npy\"), allow_pickle=True)\n",
    "    test_ids = np.load(os.path.join(INPUT_DIR, \"test_ids.npy\"), allow_pickle=True)\n",
    "    terms_df = pd.read_csv(os.path.join(INPUT_DIR, \"train_terms.tsv\"), sep=\"\\t\")\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"HATA: Dosyalar bulunamadı! Lütfen input klasörünü kontrol edin.\\n{e}\")\n",
    "    raise\n",
    "\n",
    "# Tensorlara çevir (Test verisi için)\n",
    "X_test_tensor = torch.tensor(test_emb, dtype=torch.float32).to(DEVICE)\n",
    "\n",
    "# 2. Model Mimarisi (MLP)\n",
    "class SimpleProteinModel(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes):\n",
    "        super(SimpleProteinModel, self).__init__()\n",
    "        self.layer1 = nn.Linear(input_dim, 512)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.layer2 = nn.Linear(512, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.layer2(self.dropout(self.relu(self.layer1(x))))\n",
    "\n",
    "# 3. Ana Döngü (MF, BP, CC)\n",
    "aspects = {'F': 'Molecular Function', 'P': 'Biological Process', 'C': 'Cellular Component'}\n",
    "submission_data = []\n",
    "\n",
    "print(\"\\n--- EĞİTİM ve TAHMİN SÜRECİ BAŞLIYOR ---\")\n",
    "\n",
    "for aspect_code, aspect_name in aspects.items():\n",
    "    print(f\"\\n>>> İşleniyor: {aspect_name} ({aspect_code})\")\n",
    "    \n",
    "    # Hedefleri Hazırla\n",
    "    df_aspect = terms_df[terms_df['aspect'] == aspect_code]\n",
    "    top_terms = df_aspect['term'].value_counts().head(NUM_CLASSES).index.tolist()\n",
    "    \n",
    "    term_to_idx = {term: i for i, term in enumerate(top_terms)}\n",
    "    id_map = {pid: i for i, pid in enumerate(train_ids)}\n",
    "    \n",
    "    labels = np.zeros((len(train_ids), NUM_CLASSES), dtype=np.float32)\n",
    "    valid_rows = df_aspect[df_aspect['term'].isin(top_terms) & df_aspect['EntryID'].isin(train_ids)]\n",
    "    \n",
    "    for _, row in valid_rows.iterrows():\n",
    "        if row['EntryID'] in id_map:\n",
    "            labels[id_map[row['EntryID']], term_to_idx[row['term']]] = 1.0\n",
    "            \n",
    "    # Eğitim Verisi\n",
    "    X_train = torch.tensor(train_emb, dtype=torch.float32)\n",
    "    y_train = torch.tensor(labels, dtype=torch.float32)\n",
    "    \n",
    "    train_dataset = TensorDataset(X_train, y_train)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    \n",
    "    # Model Eğitimi\n",
    "    model = SimpleProteinModel(input_dim=320, num_classes=NUM_CLASSES).to(DEVICE)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LR)\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    \n",
    "    model.train()\n",
    "    for epoch in range(EPOCHS):\n",
    "        total_loss = 0\n",
    "        for batch_X, batch_y in train_loader:\n",
    "            batch_X, batch_y = batch_X.to(DEVICE), batch_y.to(DEVICE)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_X)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "    # Test Tahmini\n",
    "    print(f\"   {aspect_name} tahmin ediliyor...\")\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        test_logits = model(X_test_tensor)\n",
    "        test_probs = torch.sigmoid(test_logits).cpu().numpy()\n",
    "        \n",
    "    for i in range(len(test_ids)):\n",
    "        pid = test_ids[i]\n",
    "        for j in range(NUM_CLASSES):\n",
    "            score = test_probs[i, j]\n",
    "            if score > 0.01: \n",
    "                submission_data.append([pid, top_terms[j], round(score, 3)])\n",
    "\n",
    "# 4. Kaydet\n",
    "print(\"\\n--- KAYDEDİLİYOR ---\")\n",
    "df_sub = pd.DataFrame(submission_data, columns=['ProteinID', 'GO_Term', 'Score'])\n",
    "output_path = os.path.join(OUTPUT_DIR, \"submission_v1_baseline.tsv\")\n",
    "\n",
    "df_sub.to_csv(output_path, sep=\"\\t\", header=False, index=False)\n",
    "print(f\"Dosya oluşturuldu: {output_path}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
